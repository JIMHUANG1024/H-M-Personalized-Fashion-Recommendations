{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad438e12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:19:07.996235Z",
     "iopub.status.busy": "2022-05-06T10:19:07.996052Z",
     "iopub.status.idle": "2022-05-06T10:19:09.393401Z",
     "shell.execute_reply": "2022-05-06T10:19:09.392966Z"
    },
    "papermill": {
     "duration": 1.405512,
     "end_time": "2022-05-06T10:19:09.395222",
     "exception": false,
     "start_time": "2022-05-06T10:19:07.989710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import catboost\n",
    "import faiss\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b5548",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:19:09.415769Z",
     "iopub.status.busy": "2022-05-06T10:19:09.415621Z",
     "iopub.status.idle": "2022-05-06T10:19:12.649951Z",
     "shell.execute_reply": "2022-05-06T10:19:12.649416Z"
    },
    "papermill": {
     "duration": 3.238985,
     "end_time": "2022-05-06T10:19:12.651414",
     "exception": false,
     "start_time": "2022-05-06T10:19:09.412429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/xuming/workspace/h-and-m-personalized-fashion-recommendations'\n",
    "output_dir = './'\n",
    "transactions = pd.read_pickle(f\"{data_dir}/transactions_train.pkl\") # 读取 transactions_train\n",
    "users = pd.read_pickle(f\"{data_dir}/users.pkl\") # 读取 users\n",
    "items = pd.read_pickle(f\"{data_dir}/items.pkl\") # 读取 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946e787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:19:12.659219Z",
     "iopub.status.busy": "2022-05-06T10:19:12.659064Z",
     "iopub.status.idle": "2022-05-06T10:19:12.663907Z",
     "shell.execute_reply": "2022-05-06T10:19:12.663468Z"
    },
    "papermill": {
     "duration": 0.008972,
     "end_time": "2022-05-06T10:19:12.664719",
     "exception": false,
     "start_time": "2022-05-06T10:19:12.655747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_type = 'CatBoost' # 使用模型 /CatBoost\n",
    "    # candidates\n",
    "    popular_num_items = 60 # 最受欢迎的前n个items\n",
    "    popular_weeks = 1 # n个week内，最受欢迎的items\n",
    "    train_weeks = 6 # 训练使用的week数\n",
    "    item2item_num_items_for_same_product_code = 12 # 每个user最大数量的items\n",
    "\n",
    "    # features\n",
    "    user_transaction_feature_weeks = 50 # user 交易表跟踪时间\n",
    "    item_transaction_feature_weeks = 16 # item 交易表跟踪时间\n",
    "    item_age_feature_weeks = 40 # 每个item的购买年龄 - n个week内\n",
    "    user_volume_feature_weeks = 50 # 每个user购买量 - n个week内\n",
    "    item_volume_feature_weeks = 20 # 每个item的被购买量 - n个week内\n",
    "    user_item_volume_feature_weeks = 16 # user-item购买量 - n个week内\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e85f34",
   "metadata": {},
   "source": [
    "## 生成candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09378511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidates(transactions: pd.DataFrame, target_users: np.ndarray, week: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    transactions\n",
    "        original transactions (user, item, week)\n",
    "    target_users\n",
    "        候选项生成的目标用户\n",
    "    week\n",
    "        距离目前n周\n",
    "    \"\"\"\n",
    "    print(f\"create candidates (week: {week})\")\n",
    "    assert len(target_users) == len(set(target_users)) # target_users中没有user的重复\n",
    "\n",
    "    def create_candidates_repurchase(\n",
    "            strategy: str,  # 策略名称\n",
    "            transactions: pd.DataFrame, # 交易数据\n",
    "            target_users: np.ndarray, # 指定user\n",
    "            week_start: int, # 开始的week\n",
    "            max_items_per_user: int=1234567890 # 每个user最大数量的items\n",
    "        ) -> pd.DataFrame:\n",
    "        tr = transactions.query(\"user in @target_users and @week_start <= week\")[['user', 'item', 'week', 'day']].drop_duplicates(ignore_index=True) # 筛选指定user和week的交易样本\n",
    "\n",
    "        gr_day = tr.groupby(['user', 'item'])['day'].min().reset_index(name='day') # user-item对的最近一天\n",
    "        gr_week = tr.groupby(['user', 'item'])['week'].min().reset_index(name='week') # user-item对的最近一周\n",
    "        gr_volume = tr.groupby(['user', 'item']).size().reset_index(name='volume') # user-item对的样本总数\n",
    "\n",
    "        gr_day['day_rank'] = gr_day.groupby('user')['day'].rank() # 每个user的day排名\n",
    "        gr_week['week_rank'] = gr_week.groupby('user')['week'].rank() # 每个user的week排名\n",
    "        gr_volume['volume_rank'] = gr_volume.groupby('user')['volume'].rank(ascending=False) # 每个user的样本总数降序排名\n",
    "\n",
    "        candidates = gr_day.merge(gr_week, on=['user', 'item']).merge(gr_volume, on=['user', 'item']) # 合并上述三种groupby user的结果\n",
    "\n",
    "        candidates['rank_meta'] = 10**9 * candidates['day_rank'] + candidates['volume_rank'] # 先按day，再按volume做rank\n",
    "        candidates['rank_meta'] = candidates.groupby('user')['rank_meta'].rank(method='min') # 相同rank_meta时都取最小值\n",
    "        \n",
    "        # 如果用于 item2item，请删除所有项目，因为使用所有项目会造成不必要的负担。\n",
    "        # 按日小、量大的词法顺序排序，只保留最前面的项目。\n",
    "        # 如果你想保留所有项目，在max_items_per_user中指定一个足够大的数字。\n",
    "        candidates = candidates.query(\"rank_meta <= @max_items_per_user\").reset_index(drop=True) # 每个user保留最大数量的items\n",
    "\n",
    "        candidates = candidates[['user', 'item', 'week_rank', 'volume_rank', 'rank_meta']].rename(columns={'week_rank': f'{strategy}_week_rank', 'volume_rank': f'{strategy}_volume_rank'})\n",
    "\n",
    "        candidates['strategy'] = strategy \n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "\n",
    "    def  create_candidates_popular(\n",
    "            transactions: pd.DataFrame, # 交易数据\n",
    "            target_users: np.ndarray, # 指定user\n",
    "            week_start: int, # 开始的week\n",
    "            num_weeks: int, # 持续week数量\n",
    "            num_items: int, # 最受欢迎的前n个items\n",
    "        ) -> pd.DataFrame:\n",
    "        '''\n",
    "        最受欢迎items排名策略\n",
    "        '''\n",
    "        tr = transactions.query(\"@week_start <= week < @week_start + @num_weeks\")[['user', 'item']].drop_duplicates(ignore_index=True) # 指定week时间段的user-item对\n",
    "        popular_items = tr['item'].value_counts().index.values[:num_items] # 交易数据中最受欢迎的前num_items个items # value_counts会自动降序排序\n",
    "        popular_items = pd.DataFrame({\n",
    "            'item': popular_items, # 最受欢迎item列表\n",
    "            'rank': range(num_items), # item rank list\n",
    "            'crossjoinkey': 1, \n",
    "        })\n",
    "\n",
    "        candidates = pd.DataFrame({\n",
    "            'user': target_users, # 指定user列表\n",
    "            'crossjoinkey': 1,\n",
    "        })\n",
    "\n",
    "        candidates = candidates.merge(popular_items, on='crossjoinkey').drop('crossjoinkey', axis=1) # 指定user表 和 最受欢迎item表 笛卡尔积\n",
    "        candidates = candidates.rename(columns={'rank': f'pop_rank'}) \n",
    "\n",
    "        candidates['strategy'] = 'pop' # 最受欢迎items策略\n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "\n",
    "    def create_candidates_category_popular(\n",
    "        transactions: pd.DataFrame, # 交易数据表\n",
    "        items: pd.DataFrame, # item表\n",
    "        base_candidates: pd.DataFrame, # 基候选项表\n",
    "        week_start: int, # 开始的week\n",
    "        num_weeks: int, # 持续week数量\n",
    "        num_items_per_category: int, # 属性值数量最多的n个属性值\n",
    "        category: str, # 属性名称\n",
    "    ) -> pd.DataFrame:\n",
    "        tr = transactions.query(\"@week_start <= week < @week_start + @num_weeks\")[['user', 'item']].drop_duplicates() # 指定week时间段的user-item对\n",
    "        tr = tr.groupby('item').size().reset_index(name='volume') # 每个item的交易样本总数\n",
    "        tr = tr.merge(items[['item', category]], on='item') # 加入item的category特征\n",
    "        tr['cat_volume_rank'] = tr.groupby(category)['volume'].rank(ascending=False, method='min') # category每个属性值的数量排序\n",
    "        tr = tr.query(\"cat_volume_rank <= @num_items_per_category\").reset_index(drop=True) # 保留属性值数量前n的样本\n",
    "        tr = tr[['item', category, 'cat_volume_rank']].reset_index(drop=True)\n",
    "\n",
    "        candidates = base_candidates[['user', 'item']].merge(items[['item', category]], on='item') # 在基候选项表中category特征\n",
    "        candidates = candidates.groupby(['user', category]).size().reset_index(name='cat_volume') # user-category对的样本数量\n",
    "        candidates = candidates.merge(tr, on=category).drop(category, axis=1)\n",
    "        candidates['strategy'] = 'cat_pop' # 各特征最受欢迎items策略\n",
    "        return candidates\n",
    "\n",
    "\n",
    "    # 生成候选项\n",
    "    # repurchase\n",
    "    candidates_repurchase = create_candidates_repurchase('repurchase', transactions, target_users, week)\n",
    "    # popular\n",
    "    candidates_popular = create_candidates_popular(transactions, target_users, week, CFG.popular_weeks, CFG.popular_num_items)\n",
    "    # item2item2\n",
    "    candidates_item2item2 = create_candidates_repurchase('item2item2', transactions, target_users, week, CFG.item2item_num_items_for_same_product_code) \n",
    "    # category popular\n",
    "    candidates_dept = create_candidates_category_popular(transactions, items, candidates_item2item2, week, 1, 6, 'department_no_idx')\n",
    "\n",
    "    def drop_common_user_item(candidates_target: pd.DataFrame, candidates_reference: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        从 candidates_target 中 drop 那些在 candidates_reference 出现过的 user-item 对\n",
    "        \"\"\"\n",
    "        tmp = candidates_reference[['user', 'item']].reset_index(drop=True) # 获取 candidates_reference 的 user-item 对\n",
    "        tmp['flag'] = 1\n",
    "        candidates = candidates_target.merge(tmp, on=['user', 'item'], how='left') # 标注出在 candidates_reference 出现过的 user-item 对\n",
    "        return candidates.query(\"flag != 1\").reset_index(drop=True).drop('flag', axis=1) # drop并返回\n",
    "    candidates_dept = drop_common_user_item(candidates_dept, candidates_repurchase) # 从 candidates_target 中 drop 那些在 candidates_reference 出现过的 user-item 对\n",
    "\n",
    "    # 合并所有 candidates\n",
    "    candidates = [\n",
    "        candidates_repurchase,\n",
    "        candidates_popular,\n",
    "        candidates_dept,\n",
    "    ]\n",
    "    candidates = pd.concat(candidates)\n",
    "    \n",
    "    # 打印有多少 candidates 样本\n",
    "    print(f\"volume: {len(candidates)}\") \n",
    "    print(f\"duplicates: {len(candidates) / len(candidates[['user', 'item']].drop_duplicates())}\") # 所有样本中 user-item 对的重复率\n",
    "\n",
    "    # 查看一下每种策略的样本数 和 占比\n",
    "    volumes = candidates.groupby('strategy').size().reset_index(name='volume').sort_values(by='volume', ascending=False).reset_index(drop=True)  \n",
    "    volumes['ratio'] = volumes['volume'] / volumes['volume'].sum()\n",
    "    print(volumes)\n",
    "\n",
    "    # 删除 meta 列名\n",
    "    meta_columns = [c for c in candidates.columns if c.endswith('_meta')]\n",
    "    return candidates.drop(meta_columns, axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978c3bcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:19:24.541958Z",
     "iopub.status.busy": "2022-05-06T10:19:24.541827Z",
     "iopub.status.idle": "2022-05-06T10:25:06.980533Z",
     "shell.execute_reply": "2022-05-06T10:25:06.979990Z"
    },
    "papermill": {
     "duration": 342.444439,
     "end_time": "2022-05-06T10:25:06.982031",
     "exception": false,
     "start_time": "2022-05-06T10:19:24.537592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# valid: week=0\n",
    "# train: week=1+CFG.train_weeks\n",
    "candidates = []\n",
    "for week in range(1+CFG.train_weeks): # 0,1,2,3,4,5,6\n",
    "    target_users = transactions.query(\"week == @week\")['user'].unique() # 某个week有交易的user\n",
    "    candidates.append(create_candidates(transactions, target_users, week+1)) # 生成candidates, 注意是上一周（week+1）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916e735",
   "metadata": {},
   "source": [
    "## 生成candidates对应的labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c04d35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:25:06.995799Z",
     "iopub.status.busy": "2022-05-06T10:25:06.995636Z",
     "iopub.status.idle": "2022-05-06T10:25:07.002746Z",
     "shell.execute_reply": "2022-05-06T10:25:07.002242Z"
    },
    "papermill": {
     "duration": 0.013202,
     "end_time": "2022-05-06T10:25:07.003535",
     "exception": false,
     "start_time": "2022-05-06T10:25:06.990333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_labels(candidates: pd.DataFrame, week: int) -> pd.DataFrame:\n",
    "    print(f\"merge labels (week: {week})\") \n",
    "    labels = transactions[transactions['week'] == week][['user', 'item']].drop_duplicates(ignore_index=True) # 交易表中某week的 user-item对\n",
    "    original_positives = len(labels) # 交易表中某week的 user-item对 数量, 即正样本数量\n",
    "    # 候选项中的user-item对在交易表中出现，则y为1，未出现则为0\n",
    "    labels['y'] = 1\n",
    "    labels = candidates.merge(labels, on=['user', 'item'], how='left')\n",
    "    labels['y'] = labels['y'].fillna(0)\n",
    "    \n",
    "    remaining_positives_total = labels[['user', 'item', 'y']].drop_duplicates(ignore_index=True)['y'].sum() # TP\n",
    "    recall = remaining_positives_total / original_positives # Recall = TP/正样本数量\n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "    # 查看一下每种策略的Recall和Accuracy\n",
    "    volumes = candidates.groupby('strategy').size().reset_index(name='volume') # 每种策略的样本数\n",
    "    remaining_positives = labels.groupby('strategy')['y'].sum().reset_index() # 每种策略的TP\n",
    "    remaining_positives = remaining_positives.merge(volumes, on='strategy') # 合并: 每种策略的 TP和样本数\n",
    "    remaining_positives['recall'] = remaining_positives['y'] / original_positives # Recall = TP/正样本数\n",
    "    remaining_positives['hit_ratio'] = remaining_positives['y'] / remaining_positives['volume'] # hit_ratio/accuracy\n",
    "    remaining_positives = remaining_positives.sort_values(by='y', ascending=False).reset_index(drop=True)\n",
    "    print(remaining_positives) # 打印每个策略的Recall和Accuracy\n",
    "\n",
    "    return labels # 返回 候选项中的每个user-item对 + candidates特征 + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b6fe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:25:07.014981Z",
     "iopub.status.busy": "2022-05-06T10:25:07.014726Z",
     "iopub.status.idle": "2022-05-06T10:25:40.040842Z",
     "shell.execute_reply": "2022-05-06T10:25:40.040241Z"
    },
    "papermill": {
     "duration": 33.033513,
     "end_time": "2022-05-06T10:25:40.042225",
     "exception": false,
     "start_time": "2022-05-06T10:25:07.008712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 每周 candidate 加上 labels\n",
    "for idx in range(len(candidates)): # 0,1,2,3,4,5,6\n",
    "    candidates[idx] = merge_labels(candidates[idx], idx) # 候选项中的每个user-item对 + candidates特征 + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96daef9b",
   "metadata": {},
   "source": [
    "## candidates数据 再处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ddcb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:25:40.055471Z",
     "iopub.status.busy": "2022-05-06T10:25:40.055341Z",
     "iopub.status.idle": "2022-05-06T10:25:45.090300Z",
     "shell.execute_reply": "2022-05-06T10:25:45.089791Z"
    },
    "papermill": {
     "duration": 5.042865,
     "end_time": "2022-05-06T10:25:45.091630",
     "exception": false,
     "start_time": "2022-05-06T10:25:40.048765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. 加入week列\n",
    "for idx in range(len(candidates)):\n",
    "    candidates[idx]['week'] = idx\n",
    "\n",
    "# 2. 最近一周，作为验证集\n",
    "candidates_valid_all = candidates[0].copy()\n",
    "\n",
    "# 3. 删除单一标签用户\n",
    "def drop_trivial_users(labels):\n",
    "    \"\"\"\n",
    "    在LightGBM xendgc和lambdarank中，只有正面或负面样本的单一标签用户需要删除，因为它们对训练没有意义，而且度量计算也是错误的。\n",
    "    \"\"\"\n",
    "    bef = len(labels)\n",
    "    df = labels[labels['user'].isin(labels[['user', 'y']].drop_duplicates().groupby('user').size().reset_index(name='sz').query(\"sz==2\").user)].reset_index(drop=True) # 保留同时用正面和负面样本的用户\n",
    "    aft = len(df)\n",
    "    print(f\"drop trivial queries: {bef} -> {aft}\") # 前后样本数量变化\n",
    "    return df\n",
    "    \n",
    "for idx in range(len(candidates)):\n",
    "    candidates[idx] = drop_trivial_users(candidates[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cac3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidates columns = ['user', 'item', 'repurchase_week_rank', 'repurchase_volume_rank', 'strategy', 'pop_rank', 'cat_volume', 'cat_volume_rank', 'y', 'week']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc49f96",
   "metadata": {},
   "source": [
    "## attach features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ec148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_embeddings(model_type: str, week: int, dim: int):\n",
    "    # 载入lfm model\n",
    "    with open(f\"{data_dir}/lfm/lfm_{model_type}_week{week}_dim{dim}_model.pkl\", 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    # 获取 user 信息\n",
    "    biases, embeddings = model.get_user_representations(None)\n",
    "    n_user = len(biases) # n个users\n",
    "    a = np.hstack([embeddings, biases.reshape(n_user, 1)]) # 合并 embeddings 和 biases\n",
    "    user_embeddings = pd.DataFrame(a, columns=[f\"user_rep_{i}\" for i in range(dim + 1)]) # 每一维都是一个特征\n",
    "    user_embeddings = pd.concat([pd.DataFrame({'user': range(n_user)}), user_embeddings], axis=1) # 获得 user_embeddings\n",
    "\n",
    "    return user_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29079a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:25:45.104440Z",
     "iopub.status.busy": "2022-05-06T10:25:45.104336Z",
     "iopub.status.idle": "2022-05-06T10:41:06.155659Z",
     "shell.execute_reply": "2022-05-06T10:41:06.155115Z"
    },
    "papermill": {
     "duration": 921.057729,
     "end_time": "2022-05-06T10:41:06.157206",
     "exception": false,
     "start_time": "2022-05-06T10:25:45.099477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def attach_features(transactions: pd.DataFrame, users: pd.DataFrame, items: pd.DataFrame, candidates: pd.DataFrame, week: int, pretrain_week: int) -> pd.DataFrame:\n",
    "    print(f\"attach features (week: {week})\")\n",
    "    n_original = len(candidates) # 候选项样本数量\n",
    "    df = candidates.copy() # 候选项df\n",
    "\n",
    "    # user静态特征, 加入年龄\n",
    "    df = df.merge(users[['user', 'age']], on='user')\n",
    "\n",
    "    # item静态特征，所有idx结尾特征\n",
    "    item_features = [c for c in items.columns if c.endswith('idx')]\n",
    "    df = df.merge(items[['item'] + item_features], on='item')\n",
    "\n",
    "    # user动态特征 (transactions)\n",
    "    week_end = week + CFG.user_transaction_feature_weeks\n",
    "    # 交易表中，每个用户在某个时间段的[price 和 sales_channel_id]的[平均值,标准差]\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby('user')[['price', 'sales_channel_id']].agg(['mean', 'std'])\n",
    "    tmp.columns = ['user_' + '_'.join(a) for a in tmp.columns.to_flat_index()] # 加入列名后缀 ，如 user_price_mean, user_price_std, user_sales_channel_id_mean, user_sales_channel_id_std\n",
    "    df = df.merge(tmp, on='user', how='left') # merge新列\n",
    "\n",
    "    # item动态特征 (transactions)\n",
    "    week_end = week + CFG.item_transaction_feature_weeks\n",
    "    # 交易表中，每个item在某个时间段的[price 和 sales_channel_id]的[平均值,标准差]\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby('item')[['price', 'sales_channel_id']].agg(['mean', 'std'])\n",
    "    tmp.columns = ['item_' + '_'.join(a) for a in tmp.columns.to_flat_index()] # 加入列名后缀\n",
    "    df = df.merge(tmp, on='item', how='left') # merge新列\n",
    "\n",
    "    # item动态特征  (user features)\n",
    "    week_end = week + CFG.item_age_feature_weeks\n",
    "    # 交易表某段week，加入年龄列\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").merge(users[['user', 'age']], on='user')\n",
    "    tmp = tmp.groupby('item')['age'].agg(['mean', 'std']) # 每个item的购买年龄的[mean和std]\n",
    "    tmp.columns = [f'age_{a}' for a in tmp.columns.to_flat_index()] # 加入列名后缀\n",
    "    df = df.merge(tmp, on='item', how='left') # merge新列\n",
    "\n",
    "    # item新鲜度特征\n",
    "    tmp = transactions.query(\"@week <= week\").groupby('item')['day'].min().reset_index(name='item_day_min') # 指定week及之前，每个item的最近一次被购买的day\n",
    "    tmp['item_day_min'] -= transactions.query(\"@week == week\")['day'].min() # item有多少天没有被购买了。\n",
    "    df = df.merge(tmp, on='item', how='left') # merge新列 \n",
    "\n",
    "    # 每个item被购买量\n",
    "    week_end = week + CFG.item_volume_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby('item').size().reset_index(name='item_volume') # 时间范围内每个item的被购买量\n",
    "    df = df.merge(tmp, on='item', how='left')\n",
    "\n",
    "    # user新鲜度特征\n",
    "    tmp = transactions.query(\"@week <= week\").groupby('user')['day'].min().reset_index(name='user_day_min') # 每个user最近一次购买的day\n",
    "    tmp['user_day_min'] -= transactions.query(\"@week == week\")['day'].min() # 该user有多少天没有购买了\n",
    "    df = df.merge(tmp, on='user', how='left')\n",
    "\n",
    "    # 每个user购买量\n",
    "    week_end = week + CFG.user_volume_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby('user').size().reset_index(name='user_volume') # 每个user购买量\n",
    "    df = df.merge(tmp, on='user', how='left') \n",
    "\n",
    "    # user-item对新鲜度特征\n",
    "    tmp = transactions.query(\"@week <= week\").groupby(['user', 'item'])['day'].min().reset_index(name='user_item_day_min') # 每个user最近一次购买某个item的day\n",
    "    tmp['user_item_day_min'] -= transactions.query(\"@week == week\")['day'].min() # 该user有多少天没有购买该item了\n",
    "    df = df.merge(tmp, on=['item', 'user'], how='left') \n",
    "\n",
    "    # user-item购买量\n",
    "    week_end = week + CFG.user_item_volume_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby(['user', 'item']).size().reset_index(name='user_item_volume')\n",
    "    df = df.merge(tmp, on=['user', 'item'], how='left')\n",
    "\n",
    "    # lfm features\n",
    "    seen_users = transactions.query(\"week >= @pretrain_week\")['user'].unique() # lfm_week 之前的 user (或者称为已知用户)\n",
    "    user_reps = calc_embeddings('i_i', pretrain_week, 16) # 获取user_embeddings\n",
    "    user_reps = user_reps.query(\"user in @seen_users\") # user 属于已知用户\n",
    "    df = df.merge(user_reps, on='user', how='left') \n",
    "\n",
    "    assert len(df) == n_original\n",
    "    return df\n",
    "\n",
    "# 生成验证特征数据\n",
    "dataset_oof = attach_features(transactions, users, items, candidates_valid_all, 1, CFG.train_weeks+1)\n",
    "# 生成训练特征数据\n",
    "datasets_train_valid = [attach_features(transactions, users, items, candidates[idx], 1+idx, CFG.train_weeks+1) for idx in range(len(candidates))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29a6b4e",
   "metadata": {},
   "source": [
    "## 划分Train/Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be4d28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:41:06.170801Z",
     "iopub.status.busy": "2022-05-06T10:41:06.170674Z",
     "iopub.status.idle": "2022-05-06T10:41:42.423578Z",
     "shell.execute_reply": "2022-05-06T10:41:42.423126Z"
    },
    "papermill": {
     "duration": 36.26077,
     "end_time": "2022-05-06T10:41:42.425367",
     "exception": false,
     "start_time": "2022-05-06T10:41:06.164597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 按week、user排序 \n",
    "for idx in range(len(datasets_train_valid)): # 循环每周 candidates df\n",
    "    datasets_train_valid[idx]['query_group'] = datasets_train_valid[idx]['week'].astype(str) + '_' + datasets_train_valid[idx]['user'].astype(str) # 新列 query_group = \"week_user\"\n",
    "    datasets_train_valid[idx] = datasets_train_valid[idx].sort_values(by='query_group').reset_index(drop=True) # 按照query_group升序\n",
    "\n",
    "# 获取 train 和 valid, train需要concat合并\n",
    "def concat_train(datasets_train_valid, begin, num):\n",
    "    train = pd.concat([datasets_train_valid[idx] for idx in range(begin, begin+num)])\n",
    "    return train\n",
    "\n",
    "train = concat_train(datasets_train_valid, 1, CFG.train_weeks) # train data\n",
    "valid = datasets_train_valid[0] # valid data\n",
    "\n",
    "# 特征列\n",
    "feature_columns = [c for c in valid.columns if c not in ['y', 'strategy', 'query_group', 'week']] # \n",
    "print(f\"feature_columns:\\n{feature_columns}\") # 打印特征列列名\n",
    "\n",
    "# 标签编码特征 #catboost需要用\n",
    "cat_feature_values = [c for c in feature_columns if c.endswith('idx')] # 标签编码特征的列名\n",
    "cat_features = [feature_columns.index(c) for c in cat_feature_values] # 该列名在列表中的位置\n",
    "print(f\"\\ncat_feature_values:\\n{cat_feature_values}\")\n",
    "print(f\"\\ncat_features:\\n{cat_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc898165",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a2a6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:41:45.639836Z",
     "iopub.status.busy": "2022-05-06T10:41:45.639725Z",
     "iopub.status.idle": "2022-05-06T10:41:45.643141Z",
     "shell.execute_reply": "2022-05-06T10:41:45.642750Z"
    },
    "papermill": {
     "duration": 0.010072,
     "end_time": "2022-05-06T10:41:45.643843",
     "exception": false,
     "start_time": "2022-05-06T10:41:45.633771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_query_group(df):\n",
    "    '''\n",
    "    user 组\n",
    "    '''\n",
    "    users = df['user'].values # users value\n",
    "    comp_seq_index, = np.concatenate(([True], users[1:]!=users[:-1], [True])).nonzero() # user_id分界处为True，首尾各加一个True。然后nonzero返回所有True的索引号\n",
    "    group = list(np.ediff1d(comp_seq_index)) # 滚动作差，得到每个group的数量，即需要返回的值\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca57bb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:41:45.655899Z",
     "iopub.status.busy": "2022-05-06T10:41:45.655781Z",
     "iopub.status.idle": "2022-05-06T14:02:45.953080Z",
     "shell.execute_reply": "2022-05-06T14:02:45.952670Z"
    },
    "papermill": {
     "duration": 12060.305311,
     "end_time": "2022-05-06T14:02:45.954897",
     "exception": false,
     "start_time": "2022-05-06T10:41:45.649586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.model_type == 'LightGBM':\n",
    "    # user group，得到每个group的数量\n",
    "    group_train = get_query_group(train) \n",
    "    group_valid = get_query_group(valid)\n",
    "\n",
    "    train_dataset = lgb.Dataset(train[feature_columns], train['y'], group=group_train)\n",
    "    valid_dataset = lgb.Dataset(valid[feature_columns], valid['y'], group=group_valid, reference=train_dataset) # 验证集需要 reference 训练集\n",
    "\n",
    "    params = {\n",
    "        'objective': 'xendcg', # 排序\n",
    "        'boosting_type': 'gbdt', # 提升树\n",
    "        'learning_rate': 1e-6, # 学习率\n",
    "        'num_leaves': 255, # leaves数\n",
    "        'min_data_in_leaf': 100, # 每个叶节点的最少样本数量。\n",
    "        'metric': 'map',\n",
    "        'eval_at': 12,\n",
    "    }\n",
    "\n",
    "    model = lgb.train(\n",
    "                        params, # 参数\n",
    "                        train_dataset, # 训练数据\n",
    "                        valid_sets=[train_dataset, valid_dataset], # 验证数据\n",
    "                        num_boost_round=1000,  # 迭代数\n",
    "                        callbacks=[lgb.early_stopping(20)] # 早停法20轮\n",
    "                    )\n",
    "\n",
    "    lgb.plot_importance(model, importance_type='gain', figsize=(8, 16)) # 特征重要度\n",
    "\n",
    "elif CFG.model_type == 'CatBoost':\n",
    "    train_dataset = catboost.Pool(data=train[feature_columns], label=train['y'], group_id=train['query_group'], cat_features=cat_features)\n",
    "    valid_dataset = catboost.Pool(data=valid[feature_columns], label=valid['y'], group_id=valid['query_group'], cat_features=cat_features)\n",
    "\n",
    "    params = {\n",
    "        'loss_function': 'YetiRank',\n",
    "        'use_best_model': True,\n",
    "        'one_hot_max_size': 300,\n",
    "        'iterations': 10000,\n",
    "    }\n",
    "    model = catboost.CatBoost(params) # 创建模型\n",
    "    model.fit(train_dataset, eval_set=valid_dataset) # 训练模型\n",
    "\n",
    "    plt.plot(model.get_evals_result()['validation']['PFound']) # 打印验证结果\n",
    "\n",
    "    # 特征重要度\n",
    "    feature_importance = model.get_feature_importance(train_dataset)\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    plt.figure(figsize=(8, 16))\n",
    "    plt.yticks(range(len(feature_columns)), np.array(feature_columns)[sorted_idx])\n",
    "    plt.barh(range(len(feature_columns)), feature_importance[sorted_idx])\n",
    "\n",
    "del train, valid, train_dataset, valid_dataset\n",
    "gc.collect()\n",
    "\n",
    "# 保存模型\n",
    "with open(f'{output_dir}/model_for_validation.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115baf96",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a205b21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T14:02:46.227921Z",
     "iopub.status.busy": "2022-05-06T14:02:46.227763Z",
     "iopub.status.idle": "2022-05-06T14:03:56.003654Z",
     "shell.execute_reply": "2022-05-06T14:03:56.003164Z"
    },
    "papermill": {
     "duration": 70.04725,
     "end_time": "2022-05-06T14:03:56.140916",
     "exception": false,
     "start_time": "2022-05-06T14:02:46.093666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = dataset_oof[['user', 'item']].reset_index(drop=True) # 验证集的 user-item对\n",
    "pred['pred'] = model.predict(dataset_oof[feature_columns]) # 验证集的 模型输出值\n",
    "\n",
    "pred = pred.groupby(['user', 'item'])['pred'].max().reset_index() # user-item对 的模型输出值 的最大值\n",
    "pred = pred.sort_values(by=['user', 'pred'], ascending=False).reset_index(drop=True).groupby('user')['item'].apply(lambda x: list(x)[:12]).reset_index() # 获取每个user 预测概率最大的前12个item\n",
    "\n",
    "gt = transactions.query(\"week == 0\").groupby('user')['item'].apply(list).reset_index().rename(columns={'item': 'gt'}) # 获取groud truth\n",
    "merged = gt.merge(pred, on='user', how='left') \n",
    "merged['item'] = merged['item'].fillna('').apply(list) # FN 填充为 \"\"\n",
    "\n",
    "merged.to_pickle(f'{output_dir}/merged_100.pkl') # 保存gt和预测结果\n",
    "dataset_oof.to_pickle(f'{output_dir}/valid_all_100.pkl') # 保存验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913199d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "\n",
    "def apk(actual, predicted, k=12):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k] # 截断12个预测值\n",
    "\n",
    "    score = 0.0 \n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]: # p not in predicted[:i] 意味着 pred不能重复出现\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0 # gt为空，返回0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "\n",
    "def mapk(actual, predicted, k=12):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted\n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "\n",
    "print('mAP@12:', mapk(merged['gt'], merged['item'])) # 计算验证集score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f4a3e9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc22c127",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用全量数据训练\n",
    "\n",
    "datasets = [attach_features(transactions, users, items, candidates[idx], 1+idx, CFG.train_weeks) for idx in range(len(candidates))]\n",
    "# 按week、user排序\n",
    "for idx in range(len(datasets)):\n",
    "    datasets[idx]['query_group'] = datasets[idx]['week'].astype(str) + '_' + datasets[idx]['user'].astype(str) # 新列 query_group = week_user\n",
    "    datasets[idx] = datasets[idx].sort_values(by='query_group').reset_index(drop=True) # 按照 query_group 排序\n",
    "\n",
    "train = concat_train(datasets, 0, CFG.train_weeks) # 从第0周开始全量数据\n",
    "\n",
    "if CFG.model_type == 'LightGBM':\n",
    "    group_train = get_query_group(train) # user 组\n",
    "    train_dataset = lgb.Dataset(train[feature_columns], train['y'], group=group_train) # 训练集\n",
    "\n",
    "    best_iteration = model.best_iteration # 训练模型的最佳迭代数\n",
    "    model = lgb.train(params, train_dataset, num_boost_round=best_iteration) # 训练模型\n",
    "    lgb.plot_importance(model, importance_type='gain', figsize=(8, 16)) # 模型特征重要度\n",
    "\n",
    "\n",
    "elif CFG.model_type == 'CatBoost':\n",
    "    train_dataset = catboost.Pool(data=train[feature_columns], label=train['y'], group_id=train['query_group'], cat_features=cat_features)\n",
    "\n",
    "    params['iterations'] = model.get_best_iteration()# 训练模型的最佳迭代数\n",
    "    params['use_best_model'] = False # 跑慢迭代数\n",
    "    model = catboost.CatBoost(params) # 创建模型/导入参数\n",
    "    model.fit(train_dataset) # 训练模型\n",
    "    # 模型特征重要度\n",
    "    feature_importance = model.get_feature_importance(train_dataset)\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    plt.figure(figsize=(8, 16))\n",
    "    plt.yticks(range(len(feature_columns)), np.array(feature_columns)[sorted_idx])\n",
    "    plt.barh(range(len(feature_columns)), feature_importance[sorted_idx])\n",
    "\n",
    "\n",
    "del train, train_dataset\n",
    "gc.collect()\n",
    "with open(f'{output_dir}/model_for_submission.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f) # 保存模型\n",
    "\n",
    "\n",
    "del datasets, dataset_oof, candidates, candidates_valid_all\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6c67f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_users = users['user'].values \n",
    "all_users # 全量用户\n",
    "preds = []\n",
    "\n",
    "# 分chunk读取, to avoid OOM\n",
    "n_split_prediction = 10\n",
    "n_chunk = (len(all_users) + n_split_prediction - 1)// n_split_prediction\n",
    "for i in range(0, len(all_users), n_chunk):\n",
    "    print(f\"chunk: {i}\")\n",
    "    target_users = all_users[i:i+n_chunk]\n",
    "\n",
    "    candidates = create_candidates(transactions, target_users, 0) # 生成候选项\n",
    "    candidates = attach_features(transactions, users, items, candidates, 0, CFG.train_weeks) # 加入附加特征\n",
    "\n",
    "    candidates['pred'] = model.predict(candidates[feature_columns]) # 模型预测\n",
    "    pred = candidates.groupby(['user', 'item'])['pred'].max().reset_index() # user-item对 的预测最大值\n",
    "    pred = pred.sort_values(by=['user', 'pred'], ascending=False).reset_index(drop=True).groupby('user')['item'].apply(lambda x: list(x)[:12]).reset_index() # 获取每个user 预测概率最大的前12个item\n",
    "    preds.append(pred)\n",
    "\n",
    "pred = pd.concat(preds).reset_index(drop=True) # 合并所有预测值\n",
    "assert len(pred) == len(all_users)\n",
    "assert np.array_equal(pred['user'].values, all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c19607",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# user原值map 和 item原值map\n",
    "mp_user = pd.read_pickle(f\"{data_dir}/mp_customer_id.pkl\")\n",
    "mp_item = pd.read_pickle(f\"{data_dir}/mp_article_id.pkl\") # \n",
    "a_user = mp_user['val'].values \n",
    "a_item = mp_item['val'].values\n",
    "\n",
    "# 调整user和item格式\n",
    "pred['customer_id'] = pred['user'].apply(lambda x: a_user[x])\n",
    "pred['prediction'] = pred['item'].apply(lambda x: list(map(lambda y: a_item[y], x)))\n",
    "pred['prediction'] = pred['prediction'].apply(lambda x: ' '.join(map(str, x))) \n",
    "\n",
    "# 输出submission.csv\n",
    "submission = pred[['customer_id', 'prediction']] \n",
    "submission.to_csv('submission.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e7a7d138e1807293d1e3f8013295cd32ce1ecfefa78d2e112676f884ff9955c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13492.067398,
   "end_time": "2022-05-06T14:03:59.094934",
   "environment_variables": {},
   "exception": true,
   "input_path": "exp0.ipynb",
   "output_path": "exp0.out.ipynb",
   "parameters": {},
   "start_time": "2022-05-06T10:19:07.027536",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
